{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/zynicide/wine-reviews/data\n",
    "\n",
    "wine_df = pd.read_csv('data/wine_reviews.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87     16933\n",
       "86     12600\n",
       "91     11359\n",
       "92      9613\n",
       "85      9530\n",
       "93      6489\n",
       "84      6480\n",
       "94      3758\n",
       "83      3025\n",
       "82      1836\n",
       "95      1535\n",
       "81       692\n",
       "96       523\n",
       "80       397\n",
       "97       229\n",
       "98        77\n",
       "99        33\n",
       "100       19\n",
       "Name: points, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_df['points'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Aromas include tropical fruit, broom, brimston...\n",
       "1    This is ripe and fruity, a wine that is smooth...\n",
       "2    Tart and snappy, the flavors of lime flesh and...\n",
       "3    Pineapple rind, lemon pith and orange blossom ...\n",
       "4    Much like the regular bottling from 2012, this...\n",
       "Name: description, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_df['description'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![google_search.png](images/google_search.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 120)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Aromas include tropical fruit, broom, brimstone and dried herb. The palate isn't overly expressive, offering unripen...\n",
       "1    This is ripe and fruity, a wine that is smooth while still structured. Firm tannins are filled out with juicy red be...\n",
       "2    Tart and snappy, the flavors of lime flesh and rind dominate. Some green pineapple pokes through, with crisp acidity...\n",
       "3    Pineapple rind, lemon pith and orange blossom start off the aromas. The palate is a bit more opulent, with notes of ...\n",
       "4    Much like the regular bottling from 2012, this comes across as rather rough and tannic, with rustic, earthy, herbal ...\n",
       "Name: description, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_df['description'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>country</th>\n",
       "      <th>description</th>\n",
       "      <th>designation</th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>province</th>\n",
       "      <th>region_1</th>\n",
       "      <th>region_2</th>\n",
       "      <th>taster_name</th>\n",
       "      <th>taster_twitter_handle</th>\n",
       "      <th>title</th>\n",
       "      <th>variety</th>\n",
       "      <th>winery</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Italy</td>\n",
       "      <td>Aromas include tropical fruit, broom, brimstone and dried herb. The palate isn't overly expressive, offering unripen...</td>\n",
       "      <td>Vulkà Bianco</td>\n",
       "      <td>87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sicily &amp; Sardinia</td>\n",
       "      <td>Etna</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kerin O’Keefe</td>\n",
       "      <td>@kerinokeefe</td>\n",
       "      <td>Nicosia 2013 Vulkà Bianco  (Etna)</td>\n",
       "      <td>White Blend</td>\n",
       "      <td>Nicosia</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>This is ripe and fruity, a wine that is smooth while still structured. Firm tannins are filled out with juicy red be...</td>\n",
       "      <td>Avidagos</td>\n",
       "      <td>87</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Douro</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Roger Voss</td>\n",
       "      <td>@vossroger</td>\n",
       "      <td>Quinta dos Avidagos 2011 Avidagos Red (Douro)</td>\n",
       "      <td>Portuguese Red</td>\n",
       "      <td>Quinta dos Avidagos</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>US</td>\n",
       "      <td>Tart and snappy, the flavors of lime flesh and rind dominate. Some green pineapple pokes through, with crisp acidity...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>87</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Paul Gregutt</td>\n",
       "      <td>@paulgwine</td>\n",
       "      <td>Rainstorm 2013 Pinot Gris (Willamette Valley)</td>\n",
       "      <td>Pinot Gris</td>\n",
       "      <td>Rainstorm</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>US</td>\n",
       "      <td>Pineapple rind, lemon pith and orange blossom start off the aromas. The palate is a bit more opulent, with notes of ...</td>\n",
       "      <td>Reserve Late Harvest</td>\n",
       "      <td>87</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>Lake Michigan Shore</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alexander Peartree</td>\n",
       "      <td>NaN</td>\n",
       "      <td>St. Julian 2013 Reserve Late Harvest Riesling (Lake Michigan Shore)</td>\n",
       "      <td>Riesling</td>\n",
       "      <td>St. Julian</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>US</td>\n",
       "      <td>Much like the regular bottling from 2012, this comes across as rather rough and tannic, with rustic, earthy, herbal ...</td>\n",
       "      <td>Vintner's Reserve Wild Child Block</td>\n",
       "      <td>87</td>\n",
       "      <td>65.0</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Willamette Valley</td>\n",
       "      <td>Paul Gregutt</td>\n",
       "      <td>@paulgwine</td>\n",
       "      <td>Sweet Cheeks 2012 Vintner's Reserve Wild Child Block Pinot Noir (Willamette Valley)</td>\n",
       "      <td>Pinot Noir</td>\n",
       "      <td>Sweet Cheeks</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   country  \\\n",
       "0           0     Italy   \n",
       "1           1  Portugal   \n",
       "2           2        US   \n",
       "3           3        US   \n",
       "4           4        US   \n",
       "\n",
       "                                                                                                               description  \\\n",
       "0  Aromas include tropical fruit, broom, brimstone and dried herb. The palate isn't overly expressive, offering unripen...   \n",
       "1  This is ripe and fruity, a wine that is smooth while still structured. Firm tannins are filled out with juicy red be...   \n",
       "2  Tart and snappy, the flavors of lime flesh and rind dominate. Some green pineapple pokes through, with crisp acidity...   \n",
       "3  Pineapple rind, lemon pith and orange blossom start off the aromas. The palate is a bit more opulent, with notes of ...   \n",
       "4  Much like the regular bottling from 2012, this comes across as rather rough and tannic, with rustic, earthy, herbal ...   \n",
       "\n",
       "                          designation  points  price           province  \\\n",
       "0                        Vulkà Bianco      87    NaN  Sicily & Sardinia   \n",
       "1                            Avidagos      87   15.0              Douro   \n",
       "2                                 NaN      87   14.0             Oregon   \n",
       "3                Reserve Late Harvest      87   13.0           Michigan   \n",
       "4  Vintner's Reserve Wild Child Block      87   65.0             Oregon   \n",
       "\n",
       "              region_1           region_2         taster_name  \\\n",
       "0                 Etna                NaN       Kerin O’Keefe   \n",
       "1                  NaN                NaN          Roger Voss   \n",
       "2    Willamette Valley  Willamette Valley        Paul Gregutt   \n",
       "3  Lake Michigan Shore                NaN  Alexander Peartree   \n",
       "4    Willamette Valley  Willamette Valley        Paul Gregutt   \n",
       "\n",
       "  taster_twitter_handle  \\\n",
       "0          @kerinokeefe   \n",
       "1            @vossroger   \n",
       "2           @paulgwine    \n",
       "3                   NaN   \n",
       "4           @paulgwine    \n",
       "\n",
       "                                                                                 title  \\\n",
       "0                                                    Nicosia 2013 Vulkà Bianco  (Etna)   \n",
       "1                                        Quinta dos Avidagos 2011 Avidagos Red (Douro)   \n",
       "2                                        Rainstorm 2013 Pinot Gris (Willamette Valley)   \n",
       "3                  St. Julian 2013 Reserve Late Harvest Riesling (Lake Michigan Shore)   \n",
       "4  Sweet Cheeks 2012 Vintner's Reserve Wild Child Block Pinot Noir (Willamette Valley)   \n",
       "\n",
       "          variety               winery rating  \n",
       "0     White Blend              Nicosia    Low  \n",
       "1  Portuguese Red  Quinta dos Avidagos    Low  \n",
       "2      Pinot Gris            Rainstorm    Low  \n",
       "3        Riesling           St. Julian    Low  \n",
       "4      Pinot Noir         Sweet Cheeks    Low  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Turning words in to features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1. Set up your model, fixing any parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(lowercase   = True,\n",
    "                             ngram_range = (1,2),\n",
    "                             stop_words  = 'english',\n",
    "                             min_df      = .01,\n",
    "                             max_features = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2. Fit your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`model.fit(X)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=0.01,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit(wine_df['description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Inpsect your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "459"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3. Create new data based on your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "review_word_counts = vectorizer.transform(wine_df['description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What words are associated with well-reviewed wines?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Low     51493\n",
       "High    33635\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_df['rating'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1. Set up your model, fixing any parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nb_classifier = MultinomialNB()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2. Fit your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`model.fit(X)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`model.fit(X, Y)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_classifier.fit(review_word_counts, wine_df['rating'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## We have coefficients now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -7.39826323,  -7.30949531,  -7.25502175,  -7.26975498,\n",
       "        -7.45244767,  -8.25120286,  -8.95430037, -11.16157528,\n",
       "        -7.26420455,  -7.30756667,  -6.7559653 ,  -4.06953068,\n",
       "        -6.76824909,  -7.30756667,  -7.32310096,  -6.32673742,\n",
       "        -6.73836308,  -6.98998913,  -6.62507334,  -6.20831233,\n",
       "        -6.4569712 ,  -6.15701903,  -7.2279687 ,  -4.90704741,\n",
       "        -7.03005171,  -6.33544574,  -7.10427199,  -3.85358064,\n",
       "        -6.84773016,  -6.99843989,  -6.82487889,  -5.92464988,\n",
       "        -7.35088053,  -6.45944339,  -7.36506516,  -7.70439863,\n",
       "        -6.32818355,  -5.95308316,  -6.79443158,  -7.18444573,\n",
       "        -9.05966088,  -9.24198244,  -6.84287578,  -4.6896506 ,\n",
       "        -6.91308004,  -6.50588483,  -7.04032327,  -6.69566716,\n",
       "        -6.86614688,  -7.34486245,  -6.51893416,  -5.44724259,\n",
       "        -5.92223616,  -4.72455117,  -6.06004726,  -6.69149614,\n",
       "        -6.62897579,  -7.50503121,  -7.16922989,  -7.22619093,\n",
       "        -6.88490916,  -5.36000636,  -6.34643889,  -5.11506313,\n",
       "        -7.39615575,  -7.63521476,  -6.84166586,  -5.31592505,\n",
       "        -7.02422928,  -6.52508262,  -7.28659355,  -7.34486245,\n",
       "        -7.06269556,  -6.99561503,  -6.73401052,  -5.18039066,\n",
       "        -7.06874702,  -6.98998913,  -6.66480051,  -7.09177183,\n",
       "        -6.9732989 ,  -5.52257378,  -7.12813947,  -6.31953796,\n",
       "        -6.32313121,  -7.00696267,  -6.66378786,  -7.03590825,\n",
       "        -6.73727316,  -6.98998913,  -8.31244648,  -5.4194466 ,\n",
       "        -5.90124303,  -6.23498057,  -4.33937789,  -6.75264119,\n",
       "        -7.05219284,  -7.29797887,  -7.20858484,  -5.8332579 ,\n",
       "        -6.83443691,  -6.85138647,  -4.98797859,  -6.95960005,\n",
       "        -7.63789213,  -5.57024914,  -6.80253816,  -6.72751699,\n",
       "        -7.42607251,  -6.4921507 ,  -6.3486521 ,  -6.09381524,\n",
       "        -6.37635885,  -6.68423847,  -7.31142768,  -7.02422928,\n",
       "        -6.82964648,  -7.07942599,  -7.12492403,  -6.77388291,\n",
       "        -6.78296356,  -5.9183863 ,  -4.86513586,  -6.6749839 ,\n",
       "        -7.11058116,  -5.73782743,  -7.0026922 ,  -6.94340537,\n",
       "        -5.69889977,  -7.93184944,  -7.50503121,  -6.67293891,\n",
       "        -6.86367164,  -6.62312782,  -6.76153014,  -7.35088053,\n",
       "        -7.29416934,  -7.88614729,  -6.87736196,  -6.80953936,\n",
       "        -5.81101053,  -4.37280831,  -7.90698137,  -7.84573775,\n",
       "        -8.52251795,  -9.25540546,  -6.20446863,  -7.13784829,\n",
       "        -4.65592712,  -6.71678727,  -6.65371728,  -5.95806822,\n",
       "        -5.47183027,  -6.34056085,  -7.79741917,  -6.99420558,\n",
       "        -6.5446562 ,  -7.11058116,  -7.05518238,  -7.43917313,\n",
       "        -7.16420896,  -7.32702254,  -5.93241314,  -5.74224777,\n",
       "        -6.30954502,  -3.99418674,  -7.14110562,  -6.10939098,\n",
       "        -7.37120643,  -6.06726751,  -7.33095955,  -5.70237669,\n",
       "        -7.04179928,  -3.35643412,  -6.60197318,  -6.32818355,\n",
       "        -6.54106875,  -7.03297569,  -8.28647099,  -6.5573146 ,\n",
       "        -6.91568082,  -7.6595732 ,  -6.59722709,  -7.08867105,\n",
       "        -6.87610961,  -7.23870226,  -7.64326849,  -4.6860025 ,\n",
       "        -7.20684116,  -6.57382874,  -3.7787723 ,  -6.80370163,\n",
       "        -5.65034837,  -6.68114408,  -5.14364586,  -5.02012456,\n",
       "        -6.5198102 ,  -7.11375073,  -6.39940135,  -7.11375073,\n",
       "        -7.07942599,  -5.16965629,  -6.65873989,  -6.21023973,\n",
       "        -6.5868643 ,  -6.72106535,  -5.02247473,  -6.75707579,\n",
       "        -7.07026562,  -6.99985532,  -7.77872704,  -6.38245179,\n",
       "        -6.29474038,  -6.03436981,  -5.40554542,  -6.31167797,\n",
       "        -6.19429043,  -5.78000359,  -6.12108702,  -6.29194513,\n",
       "        -6.88743761,  -8.79729662,  -7.77564537,  -6.91828838,\n",
       "        -7.30180297,  -6.58874047,  -6.27054286,  -7.06269556,\n",
       "        -5.6082634 ,  -5.82491171,  -7.93184944,  -7.41528492,\n",
       "        -6.04958749,  -7.47269583,  -6.16129646,  -6.3650337 ,\n",
       "        -5.43029556,  -7.79741917,  -6.44226505,  -4.76585848,\n",
       "        -6.16190902,  -5.31961314,  -5.84971369,  -7.62193435,\n",
       "        -5.76505935,  -6.13411267,  -6.80370163,  -7.06571671,\n",
       "        -8.32836194,  -7.01125145,  -7.55065737,  -6.47524413,\n",
       "        -6.76600442,  -6.91308004,  -6.91698375,  -7.03151264,\n",
       "        -5.50844834,  -6.14851868,  -5.86918453,  -5.91455121,\n",
       "        -7.42824406,  -6.67703308,  -6.23894359,  -7.13784829,\n",
       "        -6.91698375,  -6.98998913,  -6.97054408,  -7.15755336,\n",
       "        -5.63250701,  -5.94663952,  -7.05818088,  -7.09177183,\n",
       "        -6.68011475,  -6.3501303 ,  -7.2244163 ,  -6.46357733,\n",
       "        -4.8138677 ,  -7.0026922 ,  -5.73542454,  -4.72878135,\n",
       "        -4.76222432,  -6.30954502,  -6.91828838,  -5.11656997,\n",
       "        -7.34686446,  -7.31724729,  -7.06723072,  -5.99991106,\n",
       "        -8.73115682,  -5.82885649,  -6.85016621,  -7.6192993 ,\n",
       "        -6.32313121,  -4.03461145,  -7.70439863,  -6.2153977 ,\n",
       "        -5.38448602,  -5.58184545,  -7.0995661 ,  -5.57024914,\n",
       "        -6.69985565,  -6.77049881,  -7.42824406,  -7.51209837,\n",
       "        -7.11375073,  -6.07791485,  -6.86243631,  -5.68739657,\n",
       "        -6.48366169,  -6.61153348,  -6.43418746,  -4.94406231,\n",
       "        -7.10900013,  -7.12332018,  -7.70439863,  -7.86237707,\n",
       "        -8.25120286,  -7.73347045,  -7.81646737,  -6.25562733,\n",
       "        -6.66378786,  -7.59589121,  -7.93906969,  -7.90347874,\n",
       "        -6.94206758,  -7.55065737,  -7.93184944,  -6.910486  ,\n",
       "        -6.38321602,  -7.04032327,  -7.33293388,  -5.37663184,\n",
       "        -7.25502175,  -6.85138647,  -5.73903104,  -5.86872739,\n",
       "        -4.46892774,  -6.30458574,  -6.87987137,  -6.98858759,\n",
       "        -6.39707306,  -6.85628244,  -6.27054286,  -5.36441286,\n",
       "        -6.53126912,  -6.910486  ,  -7.00127275,  -4.59592626,\n",
       "        -7.80689791,  -7.25137211,  -7.46364599,  -6.56553758,\n",
       "        -7.34286445,  -6.55186994,  -6.25967319,  -6.73184132,\n",
       "        -6.68114408,  -7.4217435 ,  -6.89506153,  -5.69083366,\n",
       "        -6.67600797,  -6.43579777,  -6.53660247,  -7.75735218,\n",
       "        -7.43260137,  -6.45614849,  -6.6182805 ,  -6.62897579,\n",
       "        -5.34798764,  -6.87861587,  -5.41160342,  -7.70726807,\n",
       "        -7.0198846 ,  -5.60299376,  -6.73945419,  -6.88870423,\n",
       "        -6.29334177,  -5.92368369,  -4.68474279,  -6.41348609,\n",
       "        -6.77954865,  -6.8988954 ,  -6.33544574,  -4.90530191,\n",
       "        -6.98579039,  -6.97606133,  -5.73542454,  -6.38474624,\n",
       "        -6.71040427,  -6.60292511,  -6.11932387,  -6.61153348,\n",
       "        -6.03598924,  -6.90660752,  -5.7511475 ,  -7.02713626,\n",
       "        -6.70300857,  -7.62457636,  -4.74662712,  -6.46274917,\n",
       "        -6.08243208,  -6.94608633,  -6.37787861,  -5.73542454,\n",
       "        -7.06874702,  -4.44004006,  -5.64741366,  -6.70406175,\n",
       "        -5.82360023,  -7.32114593,  -5.30001236,  -6.98439471,\n",
       "        -6.17796866,  -8.12139124,  -6.74602595,  -6.05178048,\n",
       "        -6.66987924,  -6.83203883,  -6.43660389,  -6.86614688,\n",
       "        -6.57382874,  -5.36856167,  -7.31530365,  -6.09438782,\n",
       "        -6.75264119,  -7.36915515,  -5.21925323,  -7.07942599,\n",
       "        -7.07178653,  -8.36097352,  -7.49568534,  -7.21383419,\n",
       "        -7.85568808,  -6.84651435,  -7.68735276,  -6.62118607,\n",
       "        -7.6192993 ,  -6.56553758,  -6.67293891,  -6.4406443 ,\n",
       "        -7.34087043,  -7.01125145,  -5.08018693,  -7.38568445,\n",
       "        -7.24050244,  -6.58780195,  -3.32402092,  -7.06874702,\n",
       "        -7.17259123,  -7.41743315,  -6.98858759,  -5.91742615,\n",
       "        -7.65410871,  -7.13622359,  -6.70195649,  -6.41506337,\n",
       "        -6.4245797 ,  -6.6457332 ,  -6.56004809])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_classifier.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "coeficients = pd.Series(nb_classifier.coef_[0],\n",
    "                        index = vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2022             -11.161575\n",
       "drink 2020        -9.255405\n",
       "beautifully       -9.241982\n",
       "beautiful         -9.059661\n",
       "2020              -8.954300\n",
       "impressive        -8.797297\n",
       "opulent           -8.731157\n",
       "drink 2019        -8.522518\n",
       "velvety           -8.360974\n",
       "lovely            -8.328362\n",
       "cellar            -8.312446\n",
       "focused           -8.286471\n",
       "2019              -8.251203\n",
       "potential         -8.251203\n",
       "tightly           -8.121391\n",
       "producer          -7.939070\n",
       "purple            -7.931849\n",
       "dark chocolate    -7.931849\n",
       "layered           -7.931849\n",
       "drink 2017        -7.906981\n",
       "dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeficients.sort_values()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wine      -3.324021\n",
       "flavors   -3.356434\n",
       "fruit     -3.778772\n",
       "aromas    -3.853581\n",
       "finish    -3.994187\n",
       "palate    -4.034611\n",
       "acidity   -4.069531\n",
       "cherry    -4.339378\n",
       "drink     -4.372808\n",
       "tannins   -4.440040\n",
       "red       -4.468928\n",
       "ripe      -4.595926\n",
       "dry       -4.655927\n",
       "soft      -4.684743\n",
       "fresh     -4.686002\n",
       "berry     -4.689651\n",
       "black     -4.724551\n",
       "notes     -4.728781\n",
       "sweet     -4.746627\n",
       "oak       -4.762224\n",
       "dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeficients.sort_values(ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<h3> Your turn</h3>\n",
    "<p> New groups. \n",
    "<p> Kickstarter is a web-based platform for people to raise money for their projects. Not all projects meet their financial goals however. Your job is to discern if we can predict which projects will be successful or not based on their text description. </p>\n",
    "    \n",
    "<p><b>First step:</b> After loading the data in a new notebook, find the words associated with smallest and largest coefficients for a naive Bayes classifier of campaign success.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ks_url = 'https://github.com/nealcaren/KULeuvenBigData/blob/master/notebooks/data/kickstarter_100k.csv?raw=true'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3. Create new data based on your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Low', 'Low', 'Low', ..., 'Low', 'High', 'High'], dtype='<U4')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_classifier.predict(review_word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "wine_df['prediction']  = nb_classifier.predict(review_word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>prediction</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rating</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>High</th>\n",
       "      <td>26842</td>\n",
       "      <td>6793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Low</th>\n",
       "      <td>7954</td>\n",
       "      <td>43539</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "prediction   High    Low\n",
       "rating                  \n",
       "High        26842   6793\n",
       "Low          7954  43539"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(wine_df['rating'], wine_df['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8267667512451837"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(wine_df['rating'], wine_df['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[26842,  6793],\n",
       "       [ 7954, 43539]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x129bbfe48>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(wine_df['rating'], wine_df['prediction'])\n",
    "sns.heatmap(cm, annot=True, cmap=\"Greens\", fmt='g')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.77      0.80      0.78     33635\n",
      "         Low       0.87      0.85      0.86     51493\n",
      "\n",
      "   micro avg       0.83      0.83      0.83     85128\n",
      "   macro avg       0.82      0.82      0.82     85128\n",
      "weighted avg       0.83      0.83      0.83     85128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(wine_df['rating'], wine_df['prediction']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Precision: % of selected items that are correct \n",
    "\n",
    "Recall: % of correct items that are selected\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<h3> Your turn</h3>\n",
    "<p> How's your Kickstarter model doing? How many correct? Is it balanced?</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02441804, 0.97558196],\n",
       "       [0.32799953, 0.67200047],\n",
       "       [0.00346265, 0.99653735],\n",
       "       ...,\n",
       "       [0.09784092, 0.90215908],\n",
       "       [0.91947045, 0.08052955],\n",
       "       [0.64839775, 0.35160225]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_classifier.predict_proba(review_word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "predict_df = pd.DataFrame(nb_classifier.predict_proba(review_word_counts), \n",
    "                          columns=nb_classifier.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.024418</td>\n",
       "      <td>0.975582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.328000</td>\n",
       "      <td>0.672000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.003463</td>\n",
       "      <td>0.996537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.009818</td>\n",
       "      <td>0.990182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.017622</td>\n",
       "      <td>0.982378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       High       Low\n",
       "0  0.024418  0.975582\n",
       "1  0.328000  0.672000\n",
       "2  0.003463  0.996537\n",
       "3  0.009818  0.990182\n",
       "4  0.017622  0.982378"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "wine_df_prediction = pd.concat([wine_df, predict_df], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>79481</th>\n",
       "      <td>From a beautifully exposed southwest facing vineyard with views of the Pyrenees, this is a serious and impressive wi...</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5129</th>\n",
       "      <td>A blend of 28% Cabernet Franc, 23% Cabernet Sauvignon, 21% Malbec, 18% Petit Verdot and 10% Merlot, this is a big, b...</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25463</th>\n",
       "      <td>A blend of 28% Cabernet Franc, 23% Cabernet Sauvignon, 21% Malbec, 18% Petit Verdot and 10% Merlot, this is a big, b...</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18317</th>\n",
       "      <td>Initially closed, this elegant vibrant red slowly reveals alluring aromas of mature black-skinned fruit, French oak,...</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36344</th>\n",
       "      <td>Mature dark-skinned berry, leather, underbrush and dark spice are some of the aromas that emerge on this fantastic r...</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72251</th>\n",
       "      <td>Perfumed berry, rose, fragrant blue flower, tilled earth, baking spice and a hint of new leather are just some of th...</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10192</th>\n",
       "      <td>This is a rich, concentrated wine, powered by both tannins and very ripe fruit. It is produced from vines planted in...</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67748</th>\n",
       "      <td>Dark, rich mountain blueberry and blackberry form the core of this classically delicious Napa Valley wine from an es...</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75255</th>\n",
       "      <td>This is the second wine of Mouchão estate, a power in its own right. It is beautifully perfumed with violets and spi...</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77340</th>\n",
       "      <td>From one of the top estates in Cahors, this complex, dense wine is both structured and packed with great fruit. At t...</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49526</th>\n",
       "      <td>This Ferreirinha Douro Superior wine is made in exceptional years. The 2007 is the 16th vintage since 1960 (the prev...</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58945</th>\n",
       "      <td>A blend of 57% Cabernet Sauvignon, 14% Merlot, 13% Malbec, 11% Cabernet Franc and 5% Petit Verdot, this stunning, we...</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29754</th>\n",
       "      <td>Intense, lovely aromatics of wild berry, fragrant blue flower, crushed herb and a whiff of tilled soil lead the way....</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55591</th>\n",
       "      <td>Forest floor, ripe black fruit, truffle, leather and balsamic aromas open this bold, concentrated wine. The deliciou...</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20045</th>\n",
       "      <td>Coming from arguably the grandest of the Chablis Grand Crus, this is an impressive, rich and structured wine. It is ...</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                   description  \\\n",
       "79481  From a beautifully exposed southwest facing vineyard with views of the Pyrenees, this is a serious and impressive wi...   \n",
       "5129   A blend of 28% Cabernet Franc, 23% Cabernet Sauvignon, 21% Malbec, 18% Petit Verdot and 10% Merlot, this is a big, b...   \n",
       "25463  A blend of 28% Cabernet Franc, 23% Cabernet Sauvignon, 21% Malbec, 18% Petit Verdot and 10% Merlot, this is a big, b...   \n",
       "18317  Initially closed, this elegant vibrant red slowly reveals alluring aromas of mature black-skinned fruit, French oak,...   \n",
       "36344  Mature dark-skinned berry, leather, underbrush and dark spice are some of the aromas that emerge on this fantastic r...   \n",
       "72251  Perfumed berry, rose, fragrant blue flower, tilled earth, baking spice and a hint of new leather are just some of th...   \n",
       "10192  This is a rich, concentrated wine, powered by both tannins and very ripe fruit. It is produced from vines planted in...   \n",
       "67748  Dark, rich mountain blueberry and blackberry form the core of this classically delicious Napa Valley wine from an es...   \n",
       "75255  This is the second wine of Mouchão estate, a power in its own right. It is beautifully perfumed with violets and spi...   \n",
       "77340  From one of the top estates in Cahors, this complex, dense wine is both structured and packed with great fruit. At t...   \n",
       "49526  This Ferreirinha Douro Superior wine is made in exceptional years. The 2007 is the 16th vintage since 1960 (the prev...   \n",
       "58945  A blend of 57% Cabernet Sauvignon, 14% Merlot, 13% Malbec, 11% Cabernet Franc and 5% Petit Verdot, this stunning, we...   \n",
       "29754  Intense, lovely aromatics of wild berry, fragrant blue flower, crushed herb and a whiff of tilled soil lead the way....   \n",
       "55591  Forest floor, ripe black fruit, truffle, leather and balsamic aromas open this bold, concentrated wine. The deliciou...   \n",
       "20045  Coming from arguably the grandest of the Chablis Grand Crus, this is an impressive, rich and structured wine. It is ...   \n",
       "\n",
       "       points  \n",
       "79481      96  \n",
       "5129       94  \n",
       "25463      94  \n",
       "18317      94  \n",
       "36344      97  \n",
       "72251      96  \n",
       "10192      94  \n",
       "67748      93  \n",
       "75255      93  \n",
       "77340      94  \n",
       "49526      97  \n",
       "58945      93  \n",
       "29754      95  \n",
       "55591      94  \n",
       "20045      94  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_df_prediction.sort_values('High', ascending=False)[['description','points']].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39450</th>\n",
       "      <td>Reasonably accurate on the nose for Leyda Sauvignon Blanc, but also a little pickled smelling. Feels chunky and a li...</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16200</th>\n",
       "      <td>Lively aromas of grapefruit, white flowers and mineral lead into a light, fruity but rather simple palate that offer...</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66311</th>\n",
       "      <td>Dry, mild, dusty berry aromas are simple but correct for the variety. This feels scattered across the palate, with s...</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26592</th>\n",
       "      <td>Lemon citrus, toast, white flowers—the lead on this wine is feminine and light and, as the name suggests, feels like...</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38207</th>\n",
       "      <td>A light straw color, bright fruit aromas and a dot of sweetness paint the picture of an easy-drinking refresher. The...</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63660</th>\n",
       "      <td>Apple and mineral aromas are basic but clean, and the palate is fresh and lithe, with little to no extra weight. Fla...</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43618</th>\n",
       "      <td>A medium-bodied Bordeaux blend with sweet aromas of cherry pie and a hint of fresh sage and tarragon. Simple and str...</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58337</th>\n",
       "      <td>Here's a fresh and easy Sauvignon Blanc that would pair with easy appetizers or a salad lunch. Crisp but simple arom...</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13944</th>\n",
       "      <td>This straightforward Verdicchio opens with subdued aromas of stone fruit and citrus. The palate is a bit lean but of...</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76673</th>\n",
       "      <td>Earthy, leafy red-fruit aromas are a bit rustic and burnt-smelling. This solid, chunky Pinot has spicy oak leading s...</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45377</th>\n",
       "      <td>Earthy, leafy red-fruit aromas are a bit rustic and burnt-smelling. This solid, chunky Pinot has spicy oak leading s...</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58458</th>\n",
       "      <td>Slightly stalky, roasted aromas of earthy plum and berry lean in the direction of compost. This feels light and some...</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44783</th>\n",
       "      <td>Simple green-apple aromas are innocuous. This is fresh, easy and light on the palate. Flavors of apple and sweet gre...</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21333</th>\n",
       "      <td>Simple but solid apple and nectarine aromas are straight forward. This feels round and easy, without much acid-based...</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30861</th>\n",
       "      <td>Simple but solid apple and nectarine aromas are straight forward. This feels round and easy, without much acid-based...</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                   description  \\\n",
       "39450  Reasonably accurate on the nose for Leyda Sauvignon Blanc, but also a little pickled smelling. Feels chunky and a li...   \n",
       "16200  Lively aromas of grapefruit, white flowers and mineral lead into a light, fruity but rather simple palate that offer...   \n",
       "66311  Dry, mild, dusty berry aromas are simple but correct for the variety. This feels scattered across the palate, with s...   \n",
       "26592  Lemon citrus, toast, white flowers—the lead on this wine is feminine and light and, as the name suggests, feels like...   \n",
       "38207  A light straw color, bright fruit aromas and a dot of sweetness paint the picture of an easy-drinking refresher. The...   \n",
       "63660  Apple and mineral aromas are basic but clean, and the palate is fresh and lithe, with little to no extra weight. Fla...   \n",
       "43618  A medium-bodied Bordeaux blend with sweet aromas of cherry pie and a hint of fresh sage and tarragon. Simple and str...   \n",
       "58337  Here's a fresh and easy Sauvignon Blanc that would pair with easy appetizers or a salad lunch. Crisp but simple arom...   \n",
       "13944  This straightforward Verdicchio opens with subdued aromas of stone fruit and citrus. The palate is a bit lean but of...   \n",
       "76673  Earthy, leafy red-fruit aromas are a bit rustic and burnt-smelling. This solid, chunky Pinot has spicy oak leading s...   \n",
       "45377  Earthy, leafy red-fruit aromas are a bit rustic and burnt-smelling. This solid, chunky Pinot has spicy oak leading s...   \n",
       "58458  Slightly stalky, roasted aromas of earthy plum and berry lean in the direction of compost. This feels light and some...   \n",
       "44783  Simple green-apple aromas are innocuous. This is fresh, easy and light on the palate. Flavors of apple and sweet gre...   \n",
       "21333  Simple but solid apple and nectarine aromas are straight forward. This feels round and easy, without much acid-based...   \n",
       "30861  Simple but solid apple and nectarine aromas are straight forward. This feels round and easy, without much acid-based...   \n",
       "\n",
       "       points  \n",
       "39450      85  \n",
       "16200      86  \n",
       "66311      86  \n",
       "26592      84  \n",
       "38207      86  \n",
       "63660      86  \n",
       "43618      85  \n",
       "58337      86  \n",
       "13944      86  \n",
       "76673      86  \n",
       "45377      86  \n",
       "58458      87  \n",
       "44783      87  \n",
       "21333      86  \n",
       "30861      86  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_df_prediction.sort_values('Low', ascending=False)[['description','points']].head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<h3> Your turn</h3>\n",
    "  <p> Which <b>failed</b> Kickstarter campaign had the highest likelihood of being funded?\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What about overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(wine_df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68102"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17026"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=0.01,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(lowercase   = True,\n",
    "                             ngram_range = (1,2),\n",
    "                             stop_words  = 'english',\n",
    "                             min_df      = .01,\n",
    "                             max_features = None)\n",
    "\n",
    "vectorizer.fit(train['description'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_train = vectorizer.transform(train['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_classifier.fit(X_train, train['rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8279786203048368\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(train['rating'],\n",
    "                     nb_classifier.predict(X_train)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "test_wf         = vectorizer.transform(test['description'])\n",
    "test_prediction = nb_classifier.predict(test_wf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8256783742511453\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(test['rating'], test_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(lowercase=True,\n",
    "                             ngram_range = (1,3),\n",
    "                             stop_words = 'english',\n",
    "                             max_df = .60,\n",
    "                             min_df = 5,\n",
    "                             max_features = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76831\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit(train['description'])\n",
    "print(len(vectorizer.get_feature_names()))\n",
    "X_train = vectorizer.transform(train['description'])\n",
    "nb_classifier.fit(X_train, train['rating'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9176382485095885\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(train['rating'],\n",
    "                     nb_classifier.predict(X_train)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8882297662398685\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(accuracy_score(test['rating'],\n",
    "                     nb_classifier.predict(vectorizer.transform(test['description']))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<h3> Your turn</h3>\n",
    "<p> What happens to your model if you change some of the parameters for your vectorizer? Be sure to spit the data between train and test!\n",
    "\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What about a different model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ln_classifier = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nealcaren/anaconda3/envs/kuleuven/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(lowercase=True,\n",
    "                             ngram_range = (1,2),\n",
    "                             stop_words = 'english',\n",
    "                             min_df = .01,\n",
    "                             max_features = None)\n",
    "\n",
    "vectorizer.fit(train['description'])\n",
    "\n",
    "print(len(vectorizer.get_feature_names()))\n",
    "ln_classifier.fit(vectorizer.transform(train['description']), train['rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8826613021644004\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(train['rating'],\n",
    "                     ln_classifier.predict(vectorizer.transform(train['description']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8797133795371784\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(test['rating'],\n",
    "                     ln_classifier.predict(vectorizer.transform(test['description']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x126691160>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_prediction = ln_classifier.predict(vectorizer.transform(test['description']))\n",
    "\n",
    "cm = confusion_matrix(test['rating'], test_prediction)\n",
    "sns.heatmap(cm, annot=True, cmap=\"Greens\", fmt='g')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<h3> Your turn</h3>\n",
    "What is the out sample accuracy of a logistic regression model on your data?\n",
    "<p><code> from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What about a different model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](images/knn1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "tf_vector  = TfidfVectorizer(lowercase  =  True,\n",
    "                             ngram_range = (1,2),\n",
    "                             stop_words  = 'english',\n",
    "                             max_df      = .60,\n",
    "                             min_df      = .05,\n",
    "                             max_features = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(wine_df, test_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.6, max_features=None, min_df=0.05,\n",
       "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_vector.fit(train['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "review_tf = tf_vector.transform(train['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_classifier.fit(review_tf, train['rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "knn_prediction = knn_classifier.predict(review_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8431748766543974\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(accuracy_score(train['rating'], knn_prediction))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.84      0.74      0.79      9987\n",
      "         Low       0.85      0.91      0.88     15551\n",
      "\n",
      "   micro avg       0.84      0.84      0.84     25538\n",
      "   macro avg       0.84      0.83      0.83     25538\n",
      "weighted avg       0.84      0.84      0.84     25538\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(train['rating'], knn_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1270d0470>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(train['rating'], knn_prediction)\n",
    "sns.heatmap(cm, annot=True, cmap=\"Greens\", fmt='g')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<h3> Your turn</h3>\n",
    "<p> What does a k-nearest neigbhor for your speech dataset look like? (Don't forget to shrink your dataframe). How does the accuracy compare?\n",
    "</div>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](images/knn2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "knn_classifier = KNeighborsClassifier(n_neighbors = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# But what's the best fitting model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# old model: knn_classifier = KNeighborsClassifier(n_neighbors = 3)\n",
    "\n",
    "parameters = {'n_neighbors' : [2,3, 7],\n",
    "              'weights'      : ['distance', 'uniform']}\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(KNeighborsClassifier(), \n",
    "                    parameters, \n",
    "                    cv = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/cv.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# But what's the best fitting model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "grid.fit(review_tf,\n",
    "         train['rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "grid.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(grid.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_prediction = grid.best_estimator_.predict(review_tf)\n",
    "\n",
    "print(accuracy_score(train['rating'], train_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# But what's the best fitting model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "                     ('vectorizer' , CountVectorizer()),\n",
    "                     ('classifier' , KNeighborsClassifier())\n",
    "                    ])\n",
    "\n",
    "parameters = {'vectorizer__max_features' : [300, 500, 700],\n",
    "              'classifier__n_neighbors' : [2,3, 5] }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(pipeline,\n",
    "                           parameters,\n",
    "                           n_jobs = -1,\n",
    "                           cv = 3,\n",
    "                           verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "grid_search.fit(wine_df['description'],\n",
    "                wine_df['rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "grid_search.best_estimator_.get_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<h3> Homework</h3>\n",
    "<p> The \"data\" folder contains board games descriptions scraped from BoardGameGeeks.com. Analyze the relationship between the words in the <code>description</code> and whether or not reviewers thought it was a <code>quality_game</code>. \n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_url = 'https://raw.githubusercontent.com/nealcaren/KULeuvenBigData/master/notebooks/data/boardgames.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
